{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cec5248-3737-4fb8-803e-422e8b974f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 1. Import Libraries\n",
    "# =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SageMaker session and roles\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f09466-cc8c-45aa-b883-7d58051faad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:298: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Purchases</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>50640</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>61969</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>89262</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>105248</td>\n",
       "      <td>21</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>53550</td>\n",
       "      <td>49</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_ID  Age  Income  Purchases  Gender\n",
       "0            1   26   50640         35    Male\n",
       "1            2   18   61969         22    Male\n",
       "2            3   43   89262          2  Female\n",
       "3            4   60  105248         21    Male\n",
       "4            5   23   53550         49    Male"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================\n",
    "# 2. Load Dataset\n",
    "# =====================\n",
    "bucket = \"genmab-assessment\"\n",
    "filename = \"customer_segmentation_data.csv\"\n",
    "data_path = f\"s3://{bucket}/{filename}\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319dc8b6-42d7-4553-b920-505e70fcfb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k: 10\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 3. Preprocess & Train\n",
    "# =====================\n",
    "# Drop Customer_ID and encode Gender\n",
    "df_processed = df.drop(columns=[\"Customer_ID\"])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_processed[\"Gender\"] = encoder.fit_transform(df_processed[\"Gender\"])\n",
    "\n",
    "# Normalize all columns\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_processed)\n",
    "\n",
    "# Choose optimal k\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(scaled_data)\n",
    "    silhouette_scores.append(silhouette_score(scaled_data, km.labels_))\n",
    "\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(\"Optimal k:\", optimal_k)\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df[\"Cluster\"] = kmeans_final.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ec2150-9639-47ac-b9c5-2e69c08f0424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model: kmeans_model.pkl\n",
      "Created requirements.txt\n",
      "Successfully packaged model artifacts into model.tar.gz\n",
      "Model uploaded to S3: s3://genmab-assessment/customer-segmentation/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 4. Save Model Artifact\n",
    "# =====================\n",
    "import tarfile\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Your existing model saving code\n",
    "with open(\"kmeans_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump((kmeans_final, scaler), f)\n",
    "\n",
    "print(\"Saved trained model: kmeans_model.pkl\")\n",
    "\n",
    "# Create a requirements.txt file\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write('scikit-learn==1.3.2\\n')\n",
    "    f.write('pandas==2.0.1\\n')\n",
    "\n",
    "print(\"Created requirements.txt\")\n",
    "\n",
    "# Package the model and inference script into a tar.gz archive\n",
    "model_dir = \".\" # Set model_dir to the current directory\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"kmeans_model.pkl\")\n",
    "    tar.add(\"inference.py\")\n",
    "    tar.add(\"requirements.txt\")\n",
    "\n",
    "print(f\"Successfully packaged model artifacts into model.tar.gz\")\n",
    "\n",
    "# Upload the tar.gz archive to S3\n",
    "prefix = \"customer-segmentation\"\n",
    "model_artifact = session.upload_data(\"model.tar.gz\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"Model uploaded to S3:\", model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f583702c-b593-4d2a-9276-c54362978789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 5. Inference Script\n",
    "# =====================\n",
    "inference_code = \"\"\"\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \\\"\\\"\\\"Load model from artifact\\\"\\\"\\\"\n",
    "    with open(os.path.join(model_dir, \"kmeans_model.pkl\"), \"rb\") as f:\n",
    "        model, scaler = pickle.load(f)\n",
    "    return model, scaler\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \\\"\\\"\\\"Deserialize request\\\"\\\"\\\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        data = json.loads(request_body)\n",
    "        return np.array(data[\"instances\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported content type: \" + request_content_type)\n",
    "\n",
    "def predict_fn(input_data, model_and_scaler):\n",
    "    \\\"\\\"\\\"Run prediction\\\"\\\"\\\"\n",
    "    model, scaler = model_and_scaler\n",
    "    scaled = scaler.transform(input_data)\n",
    "    preds = model.predict(scaled)\n",
    "    return preds.tolist()\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \\\"\\\"\\\"Serialize output\\\"\\\"\\\"\n",
    "    if accept == \"application/json\":\n",
    "        return json.dumps({\"predictions\": prediction}), accept\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported accept type: \" + accept)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"inference.py\", \"w\") as f:\n",
    "    f.write(inference_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0a1527-9a13-4e56-bd55-a1ce1b1f4666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Endpoint deployed: customer-segmentation-endpoint\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 6. Deploy Model\n",
    "# =====================\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.0-1\",  # pick version compatible with SKLearn container\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "predictor = sklearn_model.deploy(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=\"customer-segmentation-endpoint\"\n",
    ")\n",
    "\n",
    "print(\"Endpoint deployed:\", predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95c1e68-99a3-4775-aa71-071276ea3a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {'predictions': [4, 4, 9]}\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 7. Test Endpoint\n",
    "# =====================\n",
    "import json\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Set the serializer and deserializer on the predictor object\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "# Select only the features used for training\n",
    "test_data = df_processed[['Age', 'Income', 'Purchases', 'Gender']].head(3)\n",
    "\n",
    "# Convert the DataFrame to a list of lists\n",
    "sample_data = test_data.values.tolist()\n",
    "\n",
    "# Wrap the data in a dictionary with the 'instances' key\n",
    "payload = {\"instances\": sample_data}\n",
    "\n",
    "# Send the JSON-formatted payload to the deployed endpoint\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "print(\"Predictions:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7270692c-887f-4a90-a564-66a875e16d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 8. Cleanup Endpoint\n",
    "# =====================\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
